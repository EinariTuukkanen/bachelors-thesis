% \documentclass[a4paper,english,titlepage,12pt]{article} 
\documentclass[english, 12pt, a4paper, sci, utf8, a-1b, online, table]{aaltothesis}
\usepackage[english]{babel}
% \usepackage[table]{xcolor}

% Some default packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}  % Should help with special characters
\usepackage{ifpdf}        % PDF check
\usepackage{fancyhdr}	  % Page number on top
\usepackage{parskip}	  % Change paragraphs with space not indent
\usepackage{pdfpages}	 % Attach PDF documents
\usepackage{setspace}	 % Control line space
\usepackage{graphicx}	 % Graphics
\usepackage{float}	     % Graphics layout
\usepackage{caption}
\usepackage{geometry}

% Neutral links
% \usepackage[pdfpagemode=None, colorlinks=true, urlcolor=blue, linkcolor=black, citecolor=black, pdfstartview=FitH]{hyperref}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}

% Source code
%\usepackage{listings}

% Sub figures
%\usepackage{subfig}
\usepackage{subcaption}
\usepackage{graphicx}

% Algorithm pseudo code
\usepackage{algorithm}
\usepackage{algorithmic}

% Natural references, e.g. Name [2001], [Name 1992]
\usepackage[square,numbers]{natbib}

% To-do comments
\usepackage[colorinlistoftodos]{todonotes}

% Table settings
% \setlength{\arrayrulewidth}{1mm}
% \setlength{\tabcolsep}{18pt}
\renewcommand{\arraystretch}{1.5}

% Inline comment
\newcommand{\todoinline}{\todo[inline,color=green!40]}

% Vectors, transposes, norms, absolutes, floor made easy
\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\trans}[1]{\ensuremath\vect{#1}^\top}
\newcommand{\norm}[1]{\ensuremath\Vert #1 \Vert}
\newcommand{\abs}[1]{\ensuremath|#1|}
\newcommand{\floor}[1]{\ensuremath\lfloor#1\rfloor}

% Domains of real and natural numbers
\newcommand{\R}{\ensuremath\mathbb{R}}
\newcommand{\N}{\ensuremath\mathbb{N}}


\degreeprogram{Engineering Physics and Mathematics}
\major{Systems and operations research}
\code{SCI3055}
\univdegree{BSc}
\thesisauthor{Einari Tuukkanen}
\thesistitle{Comparison of line search methods in unconstrained optimization}
\place{Espoo}
\date{31.8.2018}
\supervisor{Asst. Prof.\ Fabricio Oliveira}
\advisor{MSc Juho Andelmin}
\uselogo{aaltoRed}{''}
\keywords{line search, nonlinear, unconstrained, optimization, minimization, algorithm, performance}
\thesisabstract{
Your abstract in English. Keep the abstract short. The abstract explains your 
research topic, the methods you have used, and the results you obtained. In the 
PDF/A format of this thesis, in addition to the abstract page, the abstract text is 
written into the pdf file's metadata. Write here the text that goes into the 
metadata. The metadata cannot contain special characters, linebreak or paragraph 
break characters, so these must not be used here. If your abstract does not contain 
special characters and it does not require paragraphs, you may take advantage of 
the abstracttext macro (see the comment below). Otherwise, the metadata abstract 
text must be identical to the text on the abstract page.
}
\copyrighttext{Copyright \noexpand\copyright\ \number\year\ \ThesisAuthor}
{Copyright \copyright{} \number\year{} \ThesisAuthor}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

% Page numbering begins with roman numbers (titlepage parameter removes page number from first page)
\pagenumbering{roman}
\pagestyle{empty} % Hides page numbers from beginning

\makecoverpage

% \begin{titlepage}
% 	Aalto University \\
% 	School of Science \\
% 	Degree programme in Engineering Physics and Mathematics \\
	
% 	\vfill
	
% 	\begin{center}
% 		\begin{spacing}{1.75}
% 		{\LARGE Comparison of line search methods in unconstrained optimization}
% 		\end{spacing}
		
% 		Bachelor's Thesis \\
% 	    1.5.2019

% 		\vspace{3cm}

% 		{\large Einari Tuukkanen}

% 	\end{center}

% 	\vfill

% 	% Licensing
% 	The document can be stored and made available to the public on the open internet pages of Aalto University. \\\\
% 	All other rights are reserved.
% \end{titlepage}

\makecopyrightpage

% TIIVISTELMÄPOHJA
%  Lataa tiivistelmäpohja täytettäväksi osoitteesta
%  https://noppa.aalto.fi/noppa/kurssi/tfm.kand/materiaali
%  ja tallenna se PDF-muodossa nimellä tiivistelma.pdf tähän hakemistoon
%

\begin{abstractpage}[english]
  In this thesis, we are comparing the performance of 8 line search methods as part of four optimization methods. The line searches used are constant step size, Golden section, bisection, dichotomous, Fibonacci, uniform, Newton's and Armijo's searches. The optimization algorithms, in which the line searches are used for optimizing the step size, are Newton's, gradient descent, conjugate gradient, and heavy ball methods. All algorithms and their comparisons are implemented in Python.
  
  The comparison of the methods is performed separately for two unconstrained nonlinear minimization problem. The first of these is a quadratic matrix problem and the second is an entropy minimization problem. Both of the problems are solved in the domain $\R^{50}$. The performance of the methods is measured as an average of thousand runs starting from different randomized points. For the comparison metrics, we use the solution time combined with the statistics of finding the correct minimum.
  
  Before the actual performance comparison, parameters are selected for each problem, optimization algorithm, and line search combination separately. This is done by running the algorithms using ten random starting points and a set of pre-selected parameters, and selecting the parameter combination that yields the fastest average solving time.
  
  From the final performance comparison, we find that some line searches perform much more efficiently than others. In particular, constant step size, Newton's and uniform searches perform the slowest almost without exception. In turn, Armjo's, golden section and dichotomous searches perform evenly well in almost all test scenarios. The rest of the line searches perform just slightly worse than the best ones on average. All of the optimization algorithms seem to find the correct minimums almost every time regardless of the line search choice.
  
  In conclusion, we state that while there are differences between the line search methods, they are mostly expected differences. For example, the line searches that performed the best, are commonly used in literature and usually recognized as the best line searches of the bunch tested. On the other hand, the slowest searches were expected to perform poorly due to their over-simple design and straight-forward structure. However, there are some exceptions in the line search performances and thus we suggest that the line search selection could be an interesting additional parameter when optimizing a complex problem. In more general cases and simpler problems, sticking to Armijo's or golden section search is usually enough, since they provide good results and even in the rare cases where some other search would outperform them, the difference is likely to be fairly small.
\end{abstractpage}

\newpage

% Finnish avstract
\thesistitle{Viivahakujen tehokkuuden vertailu rajoittamattomassa optimoinnissa}
\supervisor{Apulaisprof.\ Fabricio Oliveira}
\advisor{DI Juho Andelmin}
\degreeprogram{Matematiikka ja systeemitieteet}
%\department{Elektroniikan ja nanotekniikan laitos}
\major{Systeemitieteet}
%% The keywords need not be separated by \spc now.
\keywords{viivahaku, optimointialgoritmi, epälineaarinen, rajoittamaton, tehokkuus, minimointi, askelkoko}
%% Abstract text
\begin{abstractpage}[finnish]
Työssä tarkastellaan kahdeksan viivahakualgoritmin tehokkuutta osana neljää tunnettua optimointialgoritmiä. Viivahakuja käytetään osana optimointialgoritmejä määrittämään kullakin iteraatiolla askelkoko eli matka, joka liikutaan algoritmin osoittamaan suuntaan. Työssä vertailtavat menetelmät ovat vakioaskelkoko, golden section, bisection, dichotomous, Fibonacci, uniform, Newtonin ja Armijon viivahakualgoritmit. Optimointialgoritmit, joiden osana viivahakuja käytetään, ovat gradient descent, conjugate gradient, heavy ball ja Newtonin menetelmät. Kaikki algoritmit sekä niiden tehokkuusvertailu toteutetaan itse Pythonilla.

Menetelmien vertailu suoritetaan erikseen kahdelle rajoittamattomalle epälineaariselle ja konveksille minimointiongelmalle, joista ensimmäinen on neliömäinen matriisimuotoinen ongelma ja toinen entropian minimointiongelma. Molempien ongelmien ulottuvuus on $\R^{50}$. Menetelmien tehokkuus mitataan tuhannen satunnaisista pisteistä aloitetun haun keskiarvona. Menetelmien tehokkuutta arvioidaan vertailemalla kuinka monesta satunnaisesta aloituspisteestä menetelmät löytävät oikean ratkaisun sekä kauanko sen löytäminen kestää.
  
Ennen varsinaista tehokkuusvertailua kullekin viivahaun ja optimointialgoritmin yhdistelmälle suoritetaan parametrien valinta kokeellisesti. Tämä tapahtuu suorittamalla tehokkuusvertailu jokaiselle algoritmille etukäteen valituilla parametriyhdistelmillä kymmenestä satunnaisesta aloituspisteestä. Parametreista valitaan varsinaiseen tehokkuusvertailuun ne, jotka yhdessä tuottavat keskiarvoltaan nopeimman suorituksen.
  
Tehokkuusvertailusta havaitaan, optimointialgoritmit päätyvät oikeisiin optimaalisiin ratkaisuihin lähes joka kerta kaikkien viivahakujen kohdalla. Tästä syystä tehokkuusvertailu suoritetaan pääasiassa ratkaisun keston perusteella. Tuloksista huomataan myös, että osa viivahauista on huomattavasti tehokkaampia kuin toiset. Erityisesti vakioaskelkoko, uniform haku sekä Newtonin viivahaku tuottavat lähes poikkeuksetta huonoimmat tulokset. Sen sijaan golden section, dichotomous ja Armijon viivahaut pärjäävät tehokkuusvertailussa tasaisen hyvin molemmissa tutkituissa ongelmissa kaikilla neljällä optimointialgoritmeillä. Loput viivahaut suoriutuvat keskimäärin vain vähän parhaita hitaammin. Optimointialgoritmit päätyvät oikeisiin optimaalisiin ratkaisuihin lähes joka kerta kaikkien viivahakujen kohdalla.
  
Tutkimuksen lopputuloksena todetaan, että viivahakujen tehokkuuksissa on eroavaisuuksia, mutta ne ovat pääasiassa odotusten mukaisia. Parhaiten menestyneet viivahaut ovat kirjallisuuslähteiden mukaan yleisesti käytettyjä ja tehokkaimpia viivahakumenetelmiä. Vastaavasti hitaimpien viivahakualgoritmien odotettiin olevan huonoimpia niiden raskaiden tai hitaasti suppenevien algoritmien vuoksi. Tutkimuksen perusteella viivahakujen vertailusta voi olla etua lähinnä erittäin monimutkaisissa ongelmissa, joissa sopiva räätälöity viivahakuvalinta voi tuottaa pienen edun useiden iteraatioiden myötä. Tutkimuksessa erot keskiverron ja parhaan menetelmän välillä ovat suhteellisen pieniä, joten tavallisesti viivahakujen valinta ongelmakohtaisesti ei kannata. Yleisesti on siis paras pysytellä tunnetuissa ja hyviksi todetuissa menetelmissä, kuten Armijon viivahaussa.

\end{abstractpage}

\newpage

% Sisällysluettelo
% \tableofcontents
\thesistableofcontents

\subsection*{Abbreviations used}

\begin{table}[H]
  \captionof{table}{Abbreviations used in this thesis, and their explanations.}
  \label{tab:abbreviations}
  \centering
  \rowcolors{2}{white}{gray!5}
  \begin{tabular}{|l|l|}
  \hline
  \rowcolor{gray!25}
  \textbf{Abbreviation} & \textbf{Explanation} \\
  \hline
  MM                    & main method, an unconstrained optimization methods                              \\
  LS                    & line search, provides step sizes for main methods                               \\
  NM                    & Newton's method                                                                 \\
  GDM                   & Gradient descent Method                                                         \\
  CGM                   & Conjugate gradient Method                                                       \\
  HBM                   & Heavy ball method                                                               \\
  CS                    & Constant search i.e. constant step size                                         \\
  GSS                   & Golden section line search                                                      \\
  BS                    & Bisection line search                                                           \\
  DS                    & Dichotomous line search                                                         \\
  FS                    & Fibonacci line search                                                           \\
  US                    & Uniform line search                                                             \\
  NS                    & Newton's line search                                                            \\
  AS                    & Armijo's line search                                                            \\
  MSS                   & Matrix square sum objective function                                            \\
  NE                    & Negative entropy objective function                                             \\
  \hline
  \end{tabular}
\end{table}


\newpage


% Aloita todellinen sivunumerointi ensimmäisestä sisältösivusta
\pagestyle{fancy}

% Tyhjennetään kentät ja muotoilu
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Sivunnumero ylhäällä oikealla
\rhead{\footnotesize \thepage} 
\pagenumbering{arabic}

% Ensimmäisellä sivulla ei kuitenkaan näytetä numeroa
\thispagestyle{empty} 


\section{Introduction}

Unconstrained nonlinear optimization has numerous real-life applications in different fields of science and technology, such as in computational biology, machine learning, and finance.
In general, we can formulate an unconstrained optimization problem as

\begin{equation}\label{eq:minfx}
    \mathrm{min}_{\vect{x} \in \R^n} f(\vect{x}),
\end{equation}

where $f : \R^n \rightarrow \R$ is called an \emph{objective function}, and we are looking for a \emph{solution} vector $\vect{x}^* \in \R^n$ that minimizes the value of $f$ such that $f(\vect{x}^*) \leq f(\vect{x})$ for all $\vect{x}\in \R^n$. There are several different optimization algorithms (or methods) that we can use to solve the problem \eqref{eq:minfx}, e.g., Newton's and conjugate gradient methods \cite{book:nonlinear_programming}. However, the performances of different optimization methods typically vary significantly for different objective functions $f$, and it can be challenging to find the best solution method for a given problem instance. For example, we may use a different method when $f$ is convex compared to when $f$ is non-convex and has multiple local optima. Moreover, some methods are feasible only when $f$ satisfies specific properties, such as being continuously differentiable up to $n$ times.

While countless real-life problems satisfy these conditions, they are often extremely complex, and finding a global minimizing solution can be difficult.
Therefore, we often use artificial problems while developing and benchmarking optimization algorithms. \cite{test_function_collection_artifical}

In general, optimization algorithms are iterative processes that harness the processing power of computers efficiently by exploiting the properties of the objective function $f$ in order for it to converge to an optimal solution.

The commonly used optimization methods typically start from a given point, choose a direction and a step size, and move to a new point. We then repeat these steps until we meet a method-specific stopping criterion. In case we do not find a minimum, we can also use additional conditions, such as maximum time limit or iteration count, to avoid infinite loops.

The step size is often computed by a separate \emph{line search} algorithm. After computing the objective function value $f(\vect{x}')$ at a given point $\vect{x}'\in \R$, and a new direction $\vect{d}\in \R^n$, the line search function takes the form $f(\vect{x}' + \lambda \vect{d})$ which is a one-dimensional function of the step size $\lambda\in \R$. Line search algorithms try to find the value of $\lambda$ that minimizes this function, i.e., they try to solve the problem
\begin{equation}\label{eq:ls}
\mathop{\text{argmin}}_{\lambda\in \R} f(\vect{x}' + \lambda \vect{d})
\end{equation}
either approximately or exactly. Thus, line search algorithms are univariate optimization methods. Similarly to the multidimensional optimization methods, there are several different algorithms for finding the optimal step size $\lambda$, and the simplest is just a constant step size for every iteration. 

In this thesis, we will investigate the effects of different line search algorithms on the overall performance of different optimization methods. The optimization methods investigated are mostly based on the algorithms represented in the Nonlinear Optimization course that was lectured in Fall 2018 and 2019 at Aalto University.

The main goal of this thesis is to investigate how different line search algorithms affect the performance of optimization methods for unconstrained nonlinear optimization problems on a general level. Also, we will analyze the performance differences caused by the different line search methods to find metrics and methods that give us the best overall view of the algorithm's performance.

\subsection{Organization of the thesis}

In section \ref{sect:theory} Theory, we will be first introducing the problems we are using to grade the optimization methods. After that, we are going through the optimization methods used and their implementations in the form of pseudocodes. Finally, we will introduce the line search methods that we are comparing, with their respective pseudocode implementations and parameter choices.

Section \ref{sect:problem_parameter_settings} describes the setup we are using to test the performances. The section includes introducing the method for choosing the values for different optimization method parameters, generating an even but random starting point distribution, and finally, randomization of parameters for one of the problems.

Results are introduced and analyzed in section \ref{sect:results}. The section also discusses the possible shortcomings and improvements that can we could make to the approach used to compare the methods in this paper. The section \ref{sect:conclusion} Conclusion provides a brief conclusion and suggests changes if one would repeat the experiments in this paper.

We also include an appendix that contains a link to the source code used, as well as tables describing the best five parameter combinations for each optimization method and line search combination.


\section{Theory}
\label{sect:theory}


\subsection{Objective functions}


\subsubsection{Choosing the objective functions}


Since we are only considering unconstrained optimization problems, one of the most significant factors affecting the performance of an optimization algorithm is the complexity of the objective function of the problem we are solving. 
In this thesis we are going to examine two different problems with convex and differentiable objective functions $f : \R^n \rightarrow \R$ of dimension $n = 50$. 

\subsubsection{Step size function}

Each optimization method involves computing a step at each iteration which is an optimization problem of the form \eqref{eq:ls}. In practice, at iteration $k$ we are at a point $\vect{x}^k$ and have computed the new direction vector $\vect{d}^k$. To simplify notation, we can define a step size function $g_{\vect{x}^k, \vect{d}^k}(\lambda) = f(\vect{x}^k + \lambda \vect{d}^k)$ for each of the objective functions. Then at each iteration $k$ of the main optimization method, a new step size function is generated with the updated point $\vect{x}^k$ and direction vector $\vect{d}^k$.

\subsubsection{Matrix square sum}
\label{sect:matrix_square_sum}

The first problem is an extension to the regular \emph{sum of squares} function, which is defined as
\begin{equation}
    f(\mathbf{x})=f(x_1, ..., x_n)=\sum_{i=1}^{n}{ix_i^2}.
\end{equation}

To make sum of squares problem more difficult to solve, we are going modify it by adding three constants $A$, $b$ and $c$. Together they form what we are going to call a \emph{matrix square sum} function, defined as
\begin{equation}
    f(\vect{x}) = \norm{A \vect{x} + \vect{b}} ^ 2 + c \norm{\vect{x}} ^ 2,
\end{equation}

where $A$ is a positive definite (PD) $n \times n$ matrix, $\vect{x}$ and $\vect{b}$ are vectors of length $n$, and the scalar $c$ is a positive constant. The problem is convex, nonlinear, unconstrained and quadratic.

While $A$ could in theory be any $m \times n$ matrix, we are limiting it to be a PD square matrix to ensure that the function is strictly convex and has a unique global minimum. The matrix $A$ is formed in a few steps by first generating an initial $n \times n$ matrix $A'$ with random values $a_{ij} \in [-0.5, 0.5]\ \forall\ i,j = 1 \dots n$. Then we attempt to transform $A$ into a PD-matrix by $A = 0.5 (A' + A'^\top)$. Finally, if  $A$ is still not PD, it is modified by the formula 
\begin{equation}
    A = A + (\abs{\lambda_{min}} + d) I,
\end{equation}
where $\lambda_{min}$ is the minimum eigenvalue of $A$ and $d$ is a constant. The value of $d$ also plays part in determining $A$'s condition number defined as $\frac{\lambda_{max}}{\lambda_{min}}$ \cite{book:numberical_linear_algebra}. Smaller values of $d$ make the function more elliptic and harder for gradient methods to solve, while larger values produce lower condition numbers and more circular gradient curves, which makes the problem easier for certain methods like gradient descent \cite{book:introduction_continuous_optimization}. The value of $d$ used in the thesis is 5, resulting in condition numbers of around 6.5.
The values of $b = (b_1,\dots,b_n) \in \R^n$ and $c\in \R$ are also randomly generated so that $b_i \in [-0.5, 0.5]$ and $c \in [-0.5, 0.5]$.

Some of the optimization methods need to compute the function's gradient and Hessian matrix at every iteration. Also, some step size algorithms require the first and second derivatives of the step size function. In this case, we can compute the gradient and the Hessian of both functions in advance. 
We should also derive the formula for calculating the correct optima so we can use it to determine whether our algorithms produce the correct solutions. Let us begin by expanding the function into a more easily differentiable form:
\begin{align*}
    f(\vect{x}) &= \norm{A \vect{x} + \vect{b}} ^ 2 + c \norm{\vect{x}} ^ 2 \\
               &= (A \vect{x} + \vect{b})^\top(A \vect{x} + \vect{b}) + c \trans{x} \vect{x} \\
               &= (\trans{x} A^\top + \trans{b})(A\vect{x} + \vect{b}) + c \trans{x} \vect{x} \\
               &= \trans{x} A^\top A \vect{x} + \vect{x} A^\top \vect{b} + \trans{b} A \vect{x} + \trans{b} \vect{b} + c \trans{x} \vect{x} \\
               &= \trans{x} A^\top A \vect{x} + 2 \trans{b} A \vect{x} + \trans{b} \vect{b} + c \trans{x} \vect{x}.
\end{align*}

Now let us calculate the gradient $\nabla f(\vect{x})$ and the Hessian matrix \textbf{H}:
\begin{align}
    \nabla f(\vect{x}) &= \nabla(\trans{x} A^\top A \vect{x}) + 2 \nabla(\trans{b} A \vect{x}) + \nabla(\trans{b} \vect{b}) + \nabla(c \trans{x} \vect{x}) \nonumber \\
    &= (\trans{x} A^\top A)^\top + A^\top A \vect{x} + 2 (\trans{b} A)^\top + c (\trans{x})^\top + c \vect{x} \nonumber \\
    &= 2 A^\top A \vect{x} + 2 A^\top \vect{b} + 2 c \vect{x} \label{eq:grad1}\\
    &\nonumber \\
    \textbf{H} &= \nabla^2 f(\vect{x}) = 2 A^\top A + 2 c I\label{eq:hess1}
\end{align}

From the gradient equation \eqref{eq:grad1} we get the necessary optimality condition
\begin{equation}
    (A^\top A + c I) \vect{x} = - A^\top \vect{b}
\end{equation}
and because the function is strictly convex (the Hessian is positive definite for all $\vect{x} \in \mathbb{R}^n$), we get the unique optimal solution
\begin{equation}
    \vect{x} = - (A^\top A + c I)^{-1} A^\top \vect{b}
\end{equation}

Let us finally form the step size function and its derivatives:
\begin{align}
    g(\lambda) =\ &f(\vect{x} + \lambda \vect{d}) = \norm{A (\vect{x} + \lambda \vect{d}) + \vect{b}} ^ 2 + c \norm{(\vect{x} + \lambda \vect{d})} ^ 2 \nonumber \\
    =\ &\lambda \trans{d} A^\top ( 2 A \vect{x} + \lambda A \vect{d} + 2 \vect{b}) + \trans{x} A^\top ( A \vect{x} + 2 \vect{b} ) + \trans{b} \vect{b} \\
    &+ c ( \trans{x} \vect{x} + 2 \lambda \trans{d} \vect{x} + \lambda ^ 2 \trans{d} \vect{d}) \nonumber \\
    g'(\lambda) =\ &\trans{d} A^\top ( 2 A \vect{x} + 2 \lambda A \vect{d} + 2 \vect{b} ) + 2 c \trans{d} ( \vect{x} + \lambda \vect{d} ) \\
    g''(\lambda) =\ &2 \trans{d} ( A^\top A \vect{d} + c \vect{d})
\end{align}


\subsubsection{Negative entropy}


The second objective function examined in this thesis is an inverted entropy maximization problem. Inverting a maximization problem gives us a corresponding minimization problem \cite{book:convex_optimization}. We call this convex minimization problem \emph{negative entropy} problem. Its objective function is of the form:
\begin{equation}\label{eq:negent}
	f(\vect{x}) = \sum_{i=1}^{n} x_i \log x_i,
\end{equation}
where $\vect{x} \in \mathbb{R}_{> 0}^n$. Even though the domain is now limited, we can still consider the problem unconstrained. However, we have to make some adjustments to our algorithms to take the positivity constraint into account.

Because the function \eqref{eq:negent} is convex, its global minimum can be found by calculating the zero point of the gradient:
\begin{align}
    \nabla_{x_i} f(\vect{x}) &= \frac{\delta}{\delta x_i} \sum_{i=1}^{n} x_i \log x_i \\
                      &= \log x_i + 1 = 0.
\end{align}

Because the $\log x_i + 1$ is truly positive, the answer is the same for every term $x_i$. Therefore we get the minima: $\log x_i = -1 \Leftrightarrow x_i = \frac{1}{e}$, for every $i = 1 \dots n$.

In addition to the gradient, we need to calculate the Hessian matrix and the one dimensional step size function $g(\lambda) = f(\vect{x} + \lambda \vect{d})$ with its first and second derivatives with regard to $\lambda$.

As we already know the gradient of $f$, the Hessian can be derived with ease since $\frac{\delta^2}{\delta x_i \delta x_j} \sum_{i=1}^{n} x_i \log x_i = 0$ for every $i, j$ where $i \neq j$. For the diagonal cases, i.e. when $i = j$, we are left with the second derivative $\frac{1}{x_i}$. Therefore we get the following Hessian matrix:
\begin{equation}
    \textbf{H} =
    \begin{bmatrix}
    x_1^{-1} & 0        & 0      & \dots  & 0      \\
    0        & x_2^{-1} & 0      & \dots  & 0      \\
    \vdots   & \vdots   & \vdots & \ddots & \vdots \\
    0        & 0        & 0      & \dots  & x_n^{-1}
\end{bmatrix}.
\end{equation}

Finally, the step size function for $\lambda$ with its first two derivatives are as follows:
\begin{align}
    g(\lambda) &= \sum_{i=1}^{n} (x_i + \lambda d_i) \log (x_i + \lambda d_i) \\
    g'(\lambda) &= \sum_{i=1}^{n} d_i (\log(x_i + \lambda d_i) + 1) \\
    g''(\lambda) &= \sum_{i=1}^{n} \frac{d_i^2}{x_i + \lambda d_i}.
\end{align}

Because the domain of the function \eqref{eq:negent} only includes the positive numbers, we have to make some changes to our starting points, parameters, and algorithms. Therefore, the iteration points must have only positive coordinates. Adding a domain limiter (see algorithm \ref{alg:domain_limiter}) to the line search algorithms prevents the iteration points from becoming negative.

\begin{algorithm}[H]
\caption{Domain Limiter}
\label{alg:domain_limiter}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $\gamma \in [0, 1]$
\WHILE{min $(\vect{x} + \lambda \vect{d}) \leq 0$}
    \STATE $\lambda = \lambda \gamma$
\ENDWHILE
\RETURN $\lambda$
\end{algorithmic}
\end{algorithm}

We call the domain limiter from all the line search methods before evaluating $f(\vect{x} + \lambda\vect{d})$ to reduce the step size to such value that it prevents evaluation of negative logarithms. In the context of this thesis, we are going to set $d = 0.99$, but it could also be added as a parameter for each of the line search functions, as some methods will require more iterations in the while-loop than others.


\subsection{Optimization methods}


In this thesis, all nonlinear unconstrained optimization methods are called \emph{main methods} to more clearly separate them from line search methods. The main methods usually follow the same formula: start from some point $x^0$, select a step direction $d^k$ and a step size $\lambda^k$, then update the next point $x^{k + 1} = x^k + \lambda^k d^k$ and repeat until we reach a method-specific termination condition. Choosing the step direction $\lambda$ is an optimization problem of its own and is solved by the line search methods introduced in section \ref{sect:line_search_methods}. \cite{book:nonlinear_programming}


\subsubsection{Newton's method}


Newton's method (see algorithm \ref{alg_newtons}) is a more general version of its line search (or univariate) version (see \ref{sect:newtonssearch}), utilizing the gradient and hessian matrix of the function instead of its first and second single derivatives. Algorithm \ref{alg_newtons} terminates if (i) the euclidean norm of the gradient $||\nabla f(x)||$ is less than some small tolerance $l > 0$, or (ii) the max iteration count $k_{\text{max}}$ is exceeded.

As an additional note, we know that Newton's method is expected to converge in exactly one iteration for quadratic problems, such as the matrix square sum function. \cite{book:introduction_continuous_optimization} 

\begin{algorithm}[H]
\caption{Newton's Method}
\label{alg_newtons}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, k = 0$, $k_{max} \in \N$, $\vect{x} = \vect{x_0} \in \R^n$, step size function $g_{\vect{x}, \vect{d}}(\lambda)$ and line search method $L$.
\WHILE{$\norm{\nabla f(\vect{x})} > l$ \AND $k < k_{max}$}
    \STATE $\vect{d} = -H(\vect{x})^{-1} \nabla f(\vect{x})$
    \STATE $\lambda = L(g_{\vect{x}, \vect{d}})$
    \STATE $\vect{x} = \vect{x} + \lambda \vect{d}$
    \STATE $k = k + 1$
\ENDWHILE
\RETURN $\lambda$
\end{algorithmic}
\end{algorithm}


\subsubsection{Gradient descent method}


Gradient descent method (algorithm \ref{alg_gradient_descent}) can be interpreted as a first-order Newton's method, as it uses only the gradient to determine the step direction. The algorithm is otherwise the same as Newton's method except for the computation of the descent direction $d$ on line 3.

In the case of the matrix square sum problem, the performance of a gradient-based method will highly depend on the ellipticity or the condition number of the problem. \cite{book:introduction_continuous_optimization}

\begin{algorithm}[H]
\caption{Gradient Descent Method}
\label{alg_gradient_descent}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, k = 0$, $k_{max} \in \N$, $\vect{x} = \vect{x_0} \in \R^n$, step size function $g_{\vect{x}, \vect{d}}(\lambda)$ and line search method $L$.
\WHILE{$\norm{\nabla f(\vect{x})} > l$ \AND $k < k_{max}$}
    \STATE $\vect{d} = -\nabla f(\vect{x})$
    \STATE $\lambda = L(g_{\vect{x}, \vect{d}})$
    \STATE $\vect{x} = \vect{x} + \lambda \vect{d}$
    \STATE $k = k + 1$
\ENDWHILE
\RETURN $\lambda$
\end{algorithmic}
\end{algorithm}


\subsubsection{Conjugate gradient method}


Conjugate gradient method (algorithm \ref{alg_conjugate_gradient}) uses information from the previous and current iterations to compute the next iteration point. To compute a good direction, it tries to obtain an approximation of the inverse Hessian using several successive gradient ratio calculations. The method should theoretically require fewer steps than a gradient descent method. \cite{book:introduction_continuous_optimization}

The algorithm involves a second loop which runs for $n$ times per every iteration inside the main loop, where $n$ is equal to the dimension of the problem (i.e., $n = \mathrm{dim}(\vect{x})$). In terms of performance comparison, a single iteration of the algorithm means one iteration in the inner loop.

\begin{algorithm}[H]
\caption{Conjugate gradient method}
\label{alg_conjugate_gradient}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, k = 0$, $k_{max} \in \N$, $\alpha \in \R$, $\vect{x} = \vect{x_0} \in \R^n$, step size function $g_{\vect{x}, \vect{d}}(\lambda)$ and line search method $L$.
\STATE $\vect{d} = -\nabla f(\vect{x})$
\WHILE{$\norm{\nabla f(\vect{x})} > l$ \AND $k < k_{max}$}
    \STATE $\vect{y} = \vect{x}$
    \FOR{$i = 1 \dots n$}
        \STATE $\lambda = L(g_{\vect{y}, \vect{d}})$
        \STATE $\vect{y_{prev}} = \vect{y}$, \ $\vect{y} = \vect{y} + \lambda \vect{d}$
        \STATE $\alpha = \frac{\nabla f(\vect{y})^2}{\nabla f(\vect{y_{prev}})^2}$
        \STATE $\vect{d} = -\nabla f(\vect{y}) + \alpha \vect{d}$
        \STATE $k = k + 1$
    \ENDFOR
    \STATE $\vect{x} = \vect{y}$, \ $\vect{d} = \nabla f(\vect{x})$
\ENDWHILE
\RETURN $\lambda$
\end{algorithmic}
\end{algorithm}


\subsubsection{Heavy ball method}


Heavy ball method (algorithm \ref{alg_heavy_ball}) is an extension of the gradient descent with an additional term in the direction vector calculation step. The name of the method comes from an analog to the physical representation of a heavy sphere in a potential field \cite{book:heavy_ball_origin}. The heavy ball method is similar to the gradient descent but it has an additional term with a multiplier $\beta \in [0, 1)$ in the descent direction $d$ on line 3 \cite{ghadimi2014global}. Note that there are multiple different variations of the method with slightly varying positioning of the multiplier, but the basic idea always remains the same \cite{ghadimi2014global} \cite{book:heavy_ball_origin}.

\begin{algorithm}[H]
\caption{Heavy Ball Method}
\label{alg_heavy_ball}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, k = 0$, $k_{max} \in \N$, $\beta \in \R$, $\vect{x} = \vect{x_{prev}} = \vect{x_0} \in \R^n$, step size function $g_{\vect{x}, \vect{d}}(\lambda)$ and line search method $L$.
\WHILE{$\norm{\nabla f(\vect{x})} > l$ \AND $k < k_{max}$}
    \STATE $\vect{d} = -\nabla f(\vect{x}) + \beta (\vect{x} - \vect{x_{prev}})$
    \STATE $\lambda = L(g_{\vect{x}, \vect{d}})$
    \STATE $\vect{x_{prev}} = \vect{x}$, \ $\vect{x} = \vect{x} + \lambda \vect{d}$
    \STATE $k = k + 1$
\ENDWHILE
\RETURN $\lambda$
\end{algorithmic}
\end{algorithm}

\begin{table}[H]
\captionof{table}{Parameters tested for heavy ball method.}
\label{tab:params_HeavyBallMethod}
\centering
\rowcolors{2}{white}{gray!5}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!25}
Parameter & Matrix Square Sum & Negative Entropy \\
\hline
$\beta_{HBM}$ & 0.1, 0.5, 1.0, 5.0, 10.0 & 0.1, 0.5, 1.0 \\
\hline
\end{tabular}
\end{table}

As seen in the parameters table (table \ref{tab:params_HeavyBallMethod}), also values outside of the typical range of $\beta \in [0, 1)$ were tested as some of them performed better in our performance tests than the more common values.


\subsection{Line search methods}
\label{sect:line_search_methods}

Most line search methods are based on a similar idea: choose a range of values and reduce it based on function evaluations until some stopping condition is satisfied. There are also approximate approaches like backtracking search, while some line search methods exploit the derivatives of the objective function.

There are also multiple small variations of the same methods available on different sources on the internet and academic papers. Different sources also often tend to suggest different recommended values for the line search parameters. However, we pick the options for the parameters ourselves with little to no comparison with existing literature. \cite{book:convex_optimization} \cite{book:nonlinear_programming} 


\subsubsection{Constant step size}


While not an actual algorithm, the use of a constant step size value $\lambda$ can be sufficient in some scenarios with some optimization methods. In this thesis, the constant step size $\lambda$ is considered as one of the search methods to provide a more comprehensive comparison.

\begin{table}[H]
\captionof{table}{Values tested for the constant step size.}
\label{tab:params_ConstantSearch}
\centering
\rowcolors{2}{white}{gray!5}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!25}
Parameter & Matrix Square Sum & Negative Entropy \\
\hline
$\lambda$ & 0.0001, 0.1, 0.25, 0.5, 0.9 & 0.1, 0.25, 0.5 \\
\hline
\end{tabular}
\end{table}

Because constant step size causes some convergence issues with some of the configurations with the test values listed in table \ref{tab:params_ConstantSearch}, a different set of values must be used for some main methods. For matrix square sum and Newton's method the test values for parameter $\lambda$ are $(0.5, 0.9, 1.0, 1.1, 1.5)$. For negative entropy different values are used for Newton's method and conjugate gradient methods: $(0.1, 0.25, 0.5, 0.9)$ and $(0.0001, 0.1, 0.15, 0.2)$, respectively.


\subsubsection{Golden section search}
\label{sect:golden_section_search}

Golden section search (algorithm \ref{alg_golden_section}) is a sequential line search meaning that it utilizes the previous iterations when calculating the next value for the step size. The name of this method comes from its unique reduction factor of $\phi = 0.618$ \cite{book:nonlinear_programming}. The parameters tested for golden section search are displayed in table \ref{tab:params_GoldenSectionSearch}.

\begin{algorithm}[H]
\caption{Golden Section Search}
\label{alg_golden_section}
\begin{algorithmic}[1]
\STATE \textbf{initialize} tolerance $l > 0$, $\alpha = 0.618$, $k = 0$, $k_{max} \in \N$, $(a, b) \in \R$.
\STATE $\lambda = a + (1 - \alpha) (b - a)$, $\mu = a + \alpha (b - a)$.
\WHILE{$b - a > l$ \AND $k < k_{max}$}
    \IF{$\theta(\lambda) > \theta(\mu)$}
        \STATE $a = \lambda$, \ $\lambda = \mu$, \ $\mu = a + \alpha (b - a)$
    \ELSE
        \STATE $b = \mu$, \ $\mu = \lambda$, \ $\lambda = a + (1 - \alpha) (b - a)$
    \ENDIF
    \STATE $k = k + 1$
\ENDWHILE
\RETURN $\lambda = \frac{a + b}{2}$
\end{algorithmic}
\end{algorithm}

\begin{table}[H]
\captionof{table}{Parameters tested for golden section search.}
\label{tab:params_GoldenSectionSearch}
\centering
\rowcolors{2}{white}{gray!5}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!25}
Parameter & Matrix Square Sum & Negative Entropy \\
\hline
$a$ & -10, -5 & -10, -5 \\
$b$ & 5, 10 & 5, 10 \\
$l$ & 1e-04, 1e-07 & 1e-04, 1e-07 \\
\hline
\end{tabular}
\end{table}


Note that the implementation above makes an error of calling the objective function twice per iteration, even though we only edit one of the limits per iteration. The mistake almost doubles the function calls in all our performance tests for the golden section search. The increase in performance after fixing this error would be around 10 \%, which is significant but not groundbreaking within this paper. 


\subsubsection{Bisection search}


Bisection search (algorithm \ref{alg_bisection}) is a line search that uses derivatives to progress. This requires the objective functions to be pseudoconvex and therefore differentiable within some closed bounds. \cite{book:nonlinear_programming}. The parameters tested for bisection search are displayed in table \ref{tab:params_BisectionSearch}.

\begin{algorithm}[H]
\caption{Bisection Search}
\label{alg_bisection}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, k = 0$, $k_{max} \in \N$, $(a, b) \in \R$.
\WHILE{$\abs{b - a} > l$ \AND $k < k_{max}$}
    \STATE $\lambda = \frac{b + a}{2}$
    \IF{$\theta'(\lambda) = 0$}
        \RETURN $\lambda$
    \ELSIF{$\theta'(\lambda) > 0$}
        \STATE $b = \lambda$
    \ELSE
        \STATE $a = \lambda$
    \ENDIF
    \STATE $k = k + 1$
\ENDWHILE
\RETURN $\lambda = \frac{a + b}{2}$
\end{algorithmic}
\end{algorithm}

\begin{table}[H]
\captionof{table}{Parameters tested for bisection search.}
\label{tab:params_BisectionSearch}
\centering
\rowcolors{2}{white}{gray!5}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!25}
Parameter & Matrix Square Sum & Negative Entropy \\
\hline
$a$ & -10, -5 & -10, -5 \\
$b$ & 5, 10 & 5, 10 \\
$l$ & 1e-04, 1e-07 & 1e-04, 1e-07 \\
\hline
\end{tabular}
\end{table}


\subsubsection{Dichotomous search}


Dichotomous search (algorithm \ref{alg_dichotomous}) is a sequential line search. It progresses by selecting some closed bounds $[a, b]$ and selecting the mid-point of the range and then selecting new bounds $\epsilon$ away from the midpoint. \cite{book:nonlinear_programming}

\begin{algorithm}[H]
\caption{Dichotomous Search}
\label{alg_dichotomous}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, k = 0$, $k_{max} \in \N$, $(a, b) \in \R$.
\WHILE{$b - a > l$ \AND $k < k_{max}$}
    \STATE $\lambda = \frac{b + a}{2} - \epsilon$, \ $\mu = \frac{b + a}{2} + \epsilon$
    \IF{$\theta(\lambda) < \theta(\mu)$}
        \STATE $b = \mu$
    \ELSE
        \STATE $a = \lambda$
    \ENDIF
    \STATE $k = k + 1$
\ENDWHILE
\RETURN $\lambda = \frac{a + b}{2}$
\end{algorithmic}
\end{algorithm}

\begin{table}[H]
\captionof{table}{Parameters tested for dichotomous search}
\label{tab:params_DichotomousSearch}
\centering
\rowcolors{2}{white}{gray!5}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!25}
Parameter & Matrix Square Sum & Negative Entropy \\
\hline
$a$ & -10, -5 & -10, -5 \\
$b$ & 5, 10 & 5, 10 \\
$\epsilon$ & 1e-07, 1e-10 & 1e-06, 1e-07 \\
$l$ & 1e-05, 1e-09 & 1e-04, 1e-05 \\
\hline
\end{tabular}
\end{table}

Initial tests show that dichotomous search has problems converging with some of the test values displayed in table \ref{tab:params_DichotomousSearch} when using Newton's method for the negative entropy problem. Because of this, we do a slight adjustment to test parameters and give $a$ a value of $-10$ for the NE+NM combination.


\subsubsection{Fibonacci search}

In many ways, Fibonacci search (algorithm \ref{alg_fibonacci}) is similar to previous sequential searches such as golden section search. It also functions over a closed bound and reduces the interval by some factor. However, instead of of a constant reduction factor, Fibonacci search calculates a new factor for each iteration using Fibonacci numbers \cite{book:nonlinear_programming}. The parameters tested for Fibonacci search are displayed in table \ref{tab:params_FibonacciSearch}.

\begin{algorithm}[H]
\caption{Fibonacci Search}
\label{alg_fibonacci}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, \epsilon > 0, k = 0$, $k_{max} \in \N$, $(a, b) \in \R, n = \mathrm{min}_n \{ \frac{b - a}{F_n} \leq l \}$.
\STATE $\lambda = a + \frac{F_{n-2}}{F_n} (b - a)$, \  $\mu = a + \frac{F_{n-1}}{F_n} (b - a)$
\WHILE{$k \leq n - 1$ \AND $k < k_{max}$}
    \IF{$\theta(\lambda) < \theta(\mu)$}
        \STATE $a = \lambda$, \ $\lambda = \mu$, $\mu = a + \frac{F_{n-k-1}}{F_{n-k}} (b - a)$
    \ELSE
        \STATE $b = \mu$, \ $\lambda = \mu$, $\lambda = a + \frac{F_{n-k-2}}{F_{n-k}} (b - a)$
    \ENDIF
    \STATE $k = k + 1$
\ENDWHILE
\STATE $\lambda = \theta(\lambda)$
\IF{$\theta(\lambda) > \theta(\mu + \epsilon)$}
    \STATE $a = \lambda$
\ELSE
    \STATE $b = \mu$
\ENDIF
\RETURN $\lambda = \frac{a + b}{2}$
\end{algorithmic}
\end{algorithm}

The $F_n$ represents $n$:th Fibonacci number. To optimize algorithm's performance, the values of $F_n$ should be pre-evaluated. After running some tests, it seems that in our case the pre-evaluation does not effect performance too much and so we decided to generate the Fibonacci numbers during run-time using a single loop algorithm of time complexity $O(n)$.

\begin{table}[H]
\captionof{table}{Parameters tested for Fibonacci search.}
\label{tab:params_FibonacciSearch}
\centering
\rowcolors{2}{white}{gray!5}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!25}
Parameter & Matrix Square Sum & Negative Entropy \\
\hline
$a$ & -10, -5 & -10, -5 \\
$b$ & 5, 10 & 5, 10 \\
$\epsilon$ & 1e-08, 1e-10, 1e-12 & 1e-08, 1e-10, 1e-12 \\
$l$ & 1e-06, 1e-10 & 1e-06, 1e-12 \\
\hline
\end{tabular}
\end{table}


\subsubsection{Uniform search}


Uniform search (algorithm \ref{alg_uniform}) divides a bounded section of a function into intervals, then chooses the interval with the lowest value and repeats the process. We may also increase the number of intervals to divide the new interval into to increase the precision of the method. The parameters tested for uniform search are displayed in table \ref{tab:params_UniformSearch}. \cite{book:nonlinear_programming}

In practice, we choose a range $[a, b]$ and split it into $n$ sections, with one section's size being $\delta = \frac{b - a}{n}$. We end up with $n + 1$ subsections, from which we pick a $a_0 + k \delta$ with lowest value, where $k = 0 \dots n$. We then increase the section count by multiplying $n$ with $m \geq 1$ and repeat the process with the new section.

\begin{algorithm}[H]
\caption{Uniform Search}
\label{alg_uniform}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, k = 0$, $(k_{max}, n) \in \N$, $(a, b, m) \in \R$
\STATE $s = \frac{b - a}{n}$, \ $p_{min} = a$
\WHILE{$s > l$ \AND $k < k_{max}$}
    \FOR{$i = 0, \dots, n$}
        \STATE $x = a + i s$
        \IF{$\theta(x) < \theta(p_{min})$}
            \STATE $p_{min} = x$
        \ENDIF
        \STATE $k = k + 1$
    \ENDFOR
    \STATE $a = p_{min} - s$, \ $b = p_{min} + s$
    \STATE $n = \floor{n m}$, \ $s = \frac{b - a}{n}$
\ENDWHILE
\RETURN $\lambda = \frac{a + b}{2}$
\end{algorithmic}
\end{algorithm}

\begin{table}[H]
\captionof{table}{Parameters tested for uniform search}
\label{tab:params_UniformSearch}
\centering
\rowcolors{2}{white}{gray!5}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!25}
Parameter & Matrix Square Sum & Negative Entropy \\
\hline
$a$ & -10, -5 & -10, -5 \\
$b$ & 5, 10 & 5, 10 \\
$n$ & 5, 10, 100 & 5, 10, 20 \\
$m$ & 1, 1.5, 2 & 1, 1.5, 2 \\
$l$ & 1e-06, 1e-08 & 1e-05, 1e-06 \\
\hline
\end{tabular}
\end{table}


\subsubsection{Newton's search}
\label{sect:newtonssearch}


Newton's search (algorithm \ref{alg_newtons}) is another line search that uses the derivatives to find the optimal step size. It exploits the function's quadratic approximations, which yields an equation of second-order differential, and therefore the objective function must be differentiable twice. \cite{book:nonlinear_programming}

In addition to the parameters listed in table \ref{tab:params_NewtonsSearch}, the max iteration count is added as an extra parameter for negative entropy function with options being either 100 or 1000 iterations. This is because the domain limiter may cause problems with the convergence while using Newton's line search.

\begin{algorithm}[H]
\caption{Newton's Search}
\label{alg_newtons_search}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, k = 0$, $k_{max} \in \N$, $\lambda \in \R$
\WHILE{$\abs{\theta'(\lambda)} > l$ \AND $k < k_{max}$}
    \STATE $\lambda = \lambda -\frac{\theta'(\lambda)}{\theta''(\lambda)}$
    \STATE $k = k + 1$
\ENDWHILE
\RETURN $\lambda$
\end{algorithmic}
\end{algorithm}

\begin{table}[H]
\captionof{table}{Parameters tested for Newton's search}
\label{tab:params_NewtonsSearch}
\centering
\rowcolors{2}{white}{gray!5}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!25}
Parameter & Matrix Square Sum & Negative Entropy \\
\hline
$\lambda$ & 0.5, 1, 5, 10 & 0.1, 0.5, 1, 5 \\
$l$ & 1e-07, 1e-08 & 1e-07, 1e-08 \\
\hline
\end{tabular}
\end{table}


\subsubsection{Armijo's search}


Armijo's search (algorithm \ref{alg_armijo}) is the only backtracking method of the algorithms included in this thesis. In this context, backtracking means reducing the initial step size on every iteration until it satisfies the stopping condition. Armijo's search is also an inexact line search meaning it only attempts to find a good enough approximation to a minimization problem instead of finding the exact solution like many other searches introduced. The parameters tested for Armijo's search are displayed in table \ref{tab:params_ArmijoSearch}. \cite{book:convex_optimization}


\begin{algorithm}[H]
\caption{Armijo's Search}
\label{alg_armijo}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $l > 0, k = 0$, $k_{max} \in \N$, $(\lambda, \alpha, \beta) \in \R$
\STATE $\theta_0 = \theta(0)$, \ $\theta'_0 = \theta'(0)$
\WHILE{$\theta(\lambda) > \theta_0 + \alpha \lambda \theta'_0$ \AND $k < k_{max}$}
    \STATE $\lambda = \lambda \beta$
    \STATE $k = k + 1$
\ENDWHILE
\RETURN $\lambda$
\end{algorithmic}
\end{algorithm}

\begin{table}[H]
\captionof{table}{Parameters tested for Armijo's search}
\label{tab:params_ArmijoSearch}
\centering
\rowcolors{2}{white}{gray!5}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!25}
Parameter & Matrix Square Sum & Negative Entropy \\
\hline
$\lambda$ & 0.9, 1, 1.1 & 0.9, 1, 1.1 \\
$\alpha$ & 0.1, 0.25, 0.5 & 0.1, 0.25, 0.5 \\
$\beta$ & 0.5, 0.75, 0.9 & 0.5, 0.75, 0.9 \\
$l$ & 1e-07 & 1e-07 \\
\hline
\end{tabular}
\end{table}


\section{Problem parameter settings}
\label{sect:problem_parameter_settings}


In this section, we are discussing the choice and effect of different parameters, such as the starting points and the actual algorithm's parameters.


\subsection{Parameter selection}
\label{sect:parameter_selection}


The optimization method's parameter selection is a whole topic of its own and not our primary focus in this thesis. Therefore a straightforward approach for picking the parameters is justified.

Using a similar setup as when comparing the performances of the methods, we can also compare the outcomes with different parameters. Using the manually chosen parameter values presented in the theory section, we can programmatically generate all the permutations for each combination of a parameter, a line search method, the main method, and an objective function.

Displaying all the results for each parameter combination would require too much space. Therefore just a concise overview of the best parameters for each setup is found from appendix \ref{sect:appendix}.


\subsection{Starting point selection}
\label{sect:starting_point_selection}


To get reliable results with the different parameter and method combinations, we need to also take into account the starting points used.

For parameter selection, we are going to use just ten starting points since the number of parameter options already increases the required iteration count by a lot. For the actual performance tests, we are going to use 1000 starting points generated similarly.

We also need to decide some range from which the points are generated. Thus, let a single starting point be $x = (x_1, x_2, \dots x_{50})$. For matrix square sum problem the points are generated so that for each point $x$ applies $x_i \in  [-10, 10]\ \forall\ i = 1 \dots 50$. Because negative entropy problem's domain is $\R_{>0}$, the same rule applies but with the bounds being $]0, 10]$ instead.

Since we are interested in overall performance, we want the starting points to be random but evenly distributed so that no two points are the same, or too close to each other. To generate a point distribution that is random but evenly packed with points, we use a method shown in algorithm \ref{alg_x0_distribution}.

\begin{algorithm}[H]
\caption{Evenly Distributed Random Starting Points}
\label{alg_x0_distribution}
\begin{algorithmic}[1]
\STATE \textbf{initialize} $(d_{min}, x_{min}, x_{max}) \in \R, (n, p) \in \N, \vect{x}_1 = \mathbf{random}_{n}(x_{min}, x_{max})$, $q = 1$
\WHILE{$i < p$}
    \STATE $\vect{y} = \mathbf{random}_{n}(x_{min}, x_{max})$
    \STATE $\mathrm{ok} =$ \TRUE
    \FOR{$j = 1 \dots q$}
        \IF{$\norm{\vect{x}_j - \vect{y}} < d_{min}$}
            \STATE $\mathrm{ok} = $ \FALSE
            \STATE \textbf{break}
        \ENDIF
    \ENDFOR
    \IF{ok}
        \STATE $q = q + 1$, \ $i = i + 1$
        \STATE $\vect{x}_q = \vect{y}$
    \ENDIF
\ENDWHILE
\RETURN $(x_1, x_2, ..., x_p)$
\end{algorithmic}
\end{algorithm}

The algorithm generates $p$ points of dimension $n$. The generated points are stored in variables $x_q, q = 1 \dots p$. The function $\mathbf{random}_n(x_{min}, x_{max})$ generates a single point of dimension $n$ filled with random values from the range $[x_{min}, x_{max}]$. On every iteration, we test to see if the newly generated point is closer than $d_{min}$ to some existing point, and if it is, we discard the point and try again.

The algorithm used is not perfect since its performance heavily depends on the $d_{min}$ value used. The minimum distance selection depends on the point count and the available point space: too high values cause the algorithm to end up on an infinite loop, and too low values do not provide the even distribution we are expecting. By trial and error, we find the $d_{min}$ values presented in the table \ref{tab:dmin_values}. After generating the points once, they are saved into a file and used for every run with the corresponding points count, thus maintaining an equal level of randomness between measurements.

\begin{table}[H]
\centering
\captionof{table}{The minimum Euclidean distances used for different objective functions on distributions of 10 and 1000 points.}
\label{tab:dmin_values}
\begin{tabular}{|r|r|r|}
\hline
\rowcolor{gray!25} 
Point count                       & MSS $d_{min}$ & NE $d_{min}$ \\ \hline
\cellcolor{gray!5}10   & 64           & 32          \\ \hline
\cellcolor{gray!5}1000 & 48           & 24          \\ \hline
\end{tabular}
\end{table}


\subsection{Randomizing objective function}


In addition to randomizing the starting points, we are going to use randomized parameters for matrix square sum objective function. We are going to randomize the values of parameters $A$, $b$, and $c$ as described in section \ref{sect:matrix_square_sum} but for each starting point separately. Again, we want to maintain the randomness between runs, and thus always give the function a random seed equal to the index of the current point. This way, the number of randomized objective functions is equal to the number of starting points used. Combined with the saving of points, we are guaranteed to get the same test scenarios for each run.

% \subsection{Comparing Performance}

% When comparing performance, there are multiple metrics we could analyze. These include the algorithm success rate, main method and line search durations, iteration count, and function calls. The metric we are going to focus on is the success rate, which shows the percentage of starting points from which the algorithm reaches the correct minimum. The solution is interpreted as correct if the first eight decimals of the found solution match the correct precalculated solution.

% Since we expect most of the algorithms to find the solution correctly most of the time, we also need a secondary comparison metric. We choose to use the total duration of the algorithm since it is often the most interesting value for real-life use cases, and it is comparable within different methods unlike, for example, total iteration count. We will also list the other recorded metrics in the results section, but they will not be taken into consideration when scoring the methods.

\section{Results}
\label{sect:results}

We measure the performances using the best parameters found for each scenario, as described in section \ref{sect:parameter_selection}. After running the performance tests, we get the results introduced in the tables \ref{tab:performance_results_MSS_NM} to \ref{tab:performance_results_NE_HBM}. Each table contains eight rows, one for each line search method, and there is a separate table for each permutation of the main methods and objective functions. The columns show the line search method used, percentage of correct solutions or the success rate $s$ (\%), solution time $t$ (ms), iteration count $k$, total function call count $f_n$, total time taken by line search algorithm alone $t_{LS}$ (ms) and iterations made by the line search $k_{LS}$. The function call count includes all calls to the function, including its gradient, step size function and its derivatives.


\begin{center}
\rowcolors{2}{white}{gray!5}
\captionof{table}{Average performances of Newton's method when minimizing matrix square sum using different line search methods and 1000 randomly generated starting points.}
\label{tab:performance_results_MSS_NM}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor{gray!25}
\multicolumn{1}{|c|}{Line Search Method} & \multicolumn{1}{c|}{$s$ (\%)} & \multicolumn{1}{c|}{$t$ (ms)} & \multicolumn{1}{c|}{$k$} & \multicolumn{1}{c|}{$f_n$} & \multicolumn{1}{c|}{$t_{LS}$ (ms)} & \multicolumn{1}{c|}{$k_{LS}$} \\
Constant search & 100.0 & 445.8 & 1.0 & 7.0 & 0.0 & 0.0 \\
Golden section search & 100.0 & 445.0 & 1.0 & 101.0 & 2.6 & 47.0 \\
Bisection search & 99.6 & 535.9 & 1.0 & 35.0 & 89.0 & 28.0 \\
Dichotomous search & 100.0 & 429.1 & 1.0 & 77.0 & 3.3 & 35.0 \\
Fibonacci search & 100.0 & 453.2 & 1.0 & 119.0 & 4.5 & 55.0 \\
Uniform search & 100.0 & 455.0 & 1.0 & 156.0 & 7.2 & 148.0 \\
Newton's search & 100.0 & 443.5 & 1.0 & 8.0 & 0.5 & 0.0 \\
Armijo's search & 100.0 & 424.2 & 1.0 & 10.0 & 0.9 & 0.0 \\
\hline
\end{tabular}
\end{center}

\begin{center}
\rowcolors{2}{white}{gray!5}
\captionof{table}{Average performances of gradient descent method when minimizing matrix square sum using different line search methods and 1000 randomly generated starting points.}
\label{tab:performance_results_MSS_GDM}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor{gray!25}
\multicolumn{1}{|c|}{Line Search Method} & \multicolumn{1}{c|}{$s$ (\%)} & \multicolumn{1}{c|}{$t$ (ms)} & \multicolumn{1}{c|}{$k$} & \multicolumn{1}{c|}{$f_n$} & \multicolumn{1}{c|}{$t_{LS}$ (ms)} & \multicolumn{1}{c|}{$k_{LS}$} \\
Constant search & 98.9 & 4681.2 & 2942.0 & 8828.9 & 141.2 & 0.0 \\
Golden section search & 99.4 & 283.7 & 33.1 & 1784.1 & 155.4 & 841.0 \\
Bisection search & 99.4 & 823.5 & 33.0 & 696.6 & 749.8 & 594.5 \\
Dichotomous search & 99.4 & 270.1 & 33.1 & 1491.1 & 142.6 & 694.4 \\
Fibonacci search & 99.4 & 354.3 & 33.1 & 2615.3 & 238.6 & 1223.5 \\
Uniform search & 99.4 & 389.2 & 33.1 & 3508.2 & 282.3 & 3372.9 \\
Newton's search & 99.4 & 453.3 & 33.1 & 241.2 & 355.7 & 35.3 \\
Armijo's search & 99.6 & 209.3 & 25.8 & 343.2 & 97.9 & 185.5 \\
\hline
\end{tabular}
\end{center}

\begin{center}
\rowcolors{2}{white}{gray!5}
\captionof{table}{Average performances of conjugate gradient method when minimizing matrix square sum using different line search methods and 1000 randomly generated starting points.}
\label{tab:performance_results_MSS_CGM}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor{gray!25}
\multicolumn{1}{|c|}{Line Search Method} & \multicolumn{1}{c|}{$s$ (\%)} & \multicolumn{1}{c|}{$t$ (ms)} & \multicolumn{1}{c|}{$k$} & \multicolumn{1}{c|}{$f_n$} & \multicolumn{1}{c|}{$t_{LS}$ (ms)} & \multicolumn{1}{c|}{$k_{LS}$} \\
Constant search & 99.6 & 989.3 & 300.1 & 1216.2 & 23.9 & 0.0 \\
Golden section search & 100.0 & 429.5 & 50.0 & 2722.9 & 198.9 & 1258.5 \\
Bisection search & 100.0 & 1154.9 & 50.0 & 1106.0 & 1003.4 & 900.0 \\
Dichotomous search & 100.0 & 426.3 & 50.0 & 2306.0 & 177.1 & 1050.0 \\
Fibonacci search & 100.0 & 517.8 & 50.0 & 4006.0 & 298.5 & 1850.0 \\
Uniform search & 100.0 & 558.2 & 50.0 & 5356.0 & 348.9 & 5100.0 \\
Newton's search & 100.0 & 552.8 & 50.6 & 364.0 & 347.5 & 35.0 \\
Armijo's search & 100.0 & 405.3 & 50.0 & 914.8 & 157.6 & 558.8 \\
\hline
\end{tabular}
\end{center}

\begin{center}
\rowcolors{2}{white}{gray!5}
\captionof{table}{Average performances of heavy ball method when minimizing matrix square sum using different line search methods and 1000 randomly generated starting points.}
\label{tab:performance_results_MSS_HBM}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor{gray!25}
\multicolumn{1}{|c|}{Line Search Method} & \multicolumn{1}{c|}{$s$ (\%)} & \multicolumn{1}{c|}{$t$ (ms)} & \multicolumn{1}{c|}{$k$} & \multicolumn{1}{c|}{$f_n$} & \multicolumn{1}{c|}{$t_{LS}$ (ms)} & \multicolumn{1}{c|}{$k_{LS}$} \\
Constant search & 98.9 & 4711.5 & 2941.9 & 8828.8 & 134.5 & 0.0 \\
Golden section search & 99.4 & 227.7 & 24.3 & 1303.4 & 127.1 & 613.8 \\
Bisection search & 99.4 & 666.6 & 24.4 & 514.9 & 607.0 & 438.8 \\
Dichotomous search & 99.4 & 209.4 & 24.4 & 904.9 & 99.3 & 414.4 \\
Fibonacci search & 99.4 & 353.2 & 32.7 & 2585.0 & 239.5 & 1209.3 \\
Uniform search & 99.4 & 322.4 & 24.4 & 2587.4 & 232.0 & 2486.9 \\
Newton's search & 99.4 & 375.1 & 24.4 & 179.6 & 295.0 & 26.4 \\
Armijo's search & 99.6 & 217.3 & 25.8 & 343.1 & 99.6 & 185.4 \\
\hline
\end{tabular}
\end{center}


\begin{center}
\rowcolors{2}{white}{gray!5}
\captionof{table}{Average performances of Newton's method when minimizing negative entropy using different line search methods and 1000 randomly generated starting points.}
\label{tab:performance_results_NE_NM}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor{gray!25}
\multicolumn{1}{|c|}{Line Search Method} & \multicolumn{1}{c|}{$s$ (\%)} & \multicolumn{1}{c|}{$t$ (ms)} & \multicolumn{1}{c|}{$k$} & \multicolumn{1}{c|}{$f_n$} & \multicolumn{1}{c|}{$t_{LS}$ (ms)} & \multicolumn{1}{c|}{$k_{LS}$} \\
Constant search & 100.0 & 3718.1 & 10.7 & 45.7 & 1.0 & 0.0 \\
Golden section search & 100.0 & 2038.3 & 5.6 & 426.9 & 29.4 & 200.7 \\
Bisection search & 100.0 & 2191.6 & 5.6 & 116.9 & 161.4 & 91.5 \\
Dichotomous search & 100.0 & 2231.2 & 6.0 & 265.5 & 52.0 & 119.2 \\
Fibonacci search & 100.0 & 2100.7 & 5.6 & 738.5 & 59.0 & 350.9 \\
Uniform search & 100.0 & 2316.0 & 6.1 & 510.3 & 132.5 & 476.7 \\
Newton's search & 100.0 & 2266.0 & 5.7 & 192.6 & 348.6 & 53.7 \\
Armijo's search & 100.0 & 2206.9 & 6.3 & 52.6 & 3.1 & 5.5 \\
\hline
\end{tabular}
\end{center}

\begin{center}
\rowcolors{2}{white}{gray!5}
\captionof{table}{Average performances of gradient descent method when minimizing negative entropy using different line search methods and 1000 randomly generated starting points.}
\label{tab:performance_results_NE_GDM}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor{gray!25}
\multicolumn{1}{|c|}{Line Search Method} & \multicolumn{1}{c|}{$s$ (\%)} & \multicolumn{1}{c|}{$t$ (ms)} & \multicolumn{1}{c|}{$k$} & \multicolumn{1}{c|}{$f_n$} & \multicolumn{1}{c|}{$t_{LS}$ (ms)} & \multicolumn{1}{c|}{$k_{LS}$} \\
Constant search & 100.0 & 13.1 & 16.3 & 51.8 & 0.8 & 0.0 \\
Golden section search & 100.0 & 53.4 & 8.6 & 400.1 & 47.9 & 185.7 \\
Bisection search & 100.0 & 92.2 & 8.6 & 158.4 & 87.0 & 129.7 \\
Dichotomous search & 100.0 & 54.7 & 8.6 & 289.3 & 48.9 & 130.2 \\
Fibonacci search & 100.0 & 58.4 & 8.6 & 610.8 & 52.6 & 282.4 \\
Uniform search & 100.0 & 121.8 & 10.9 & 889.6 & 114.6 & 842.8 \\
Newton's search & 100.0 & 458.8 & 8.8 & 687.6 & 453.8 & 216.5 \\
Armijo's search & 100.0 & 11.6 & 10.2 & 96.5 & 5.6 & 32.0 \\
\hline
\end{tabular}
\end{center}

\begin{center}
\rowcolors{2}{white}{gray!5}
\captionof{table}{Average performances of conjugate gradient method when minimizing negative entropy using different line search methods and 1000 randomly generated starting points.}
\label{tab:performance_results_NE_CGM}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor{gray!25}
\multicolumn{1}{|c|}{Line Search Method} & \multicolumn{1}{c|}{$s$ (\%)} & \multicolumn{1}{c|}{$t$ (ms)} & \multicolumn{1}{c|}{$k$} & \multicolumn{1}{c|}{$f_n$} & \multicolumn{1}{c|}{$t_{LS}$ (ms)} & \multicolumn{1}{c|}{$k_{LS}$} \\
Constant search & 99.9 & 4393.4 & 163.2 & 663.3 & 4248.0 & 0.0 \\
Golden section search & 100.0 & 539.7 & 100.0 & 4681.2 & 451.2 & 2136.6 \\
Bisection search & 100.0 & 759.5 & 100.0 & 1228.3 & 672.9 & 777.8 \\
Dichotomous search & 100.0 & 576.4 & 102.7 & 3496.1 & 485.5 & 1538.6 \\
Fibonacci search & 100.0 & 623.7 & 100.0 & 7155.1 & 534.0 & 3273.5 \\
Uniform search & 100.0 & 6371.1 & 100.0 & 8208.0 & 6277.0 & 7700.0 \\
Newton's search & 100.0 & 2801.7 & 100.0 & 4147.0 & 2716.9 & 1213.0 \\
Armijo's search & 100.0 & 198.4 & 99.8 & 1119.4 & 112.0 & 412.8 \\
\hline
\end{tabular}
\end{center}

\begin{center}
\rowcolors{2}{white}{gray!5}
\captionof{table}{Average performances of heavy ball method when minimizing negative entropy using different line search methods and 1000 randomly generated starting points.}
\label{tab:performance_results_NE_HBM}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor{gray!25}
\multicolumn{1}{|c|}{Line Search Method} & \multicolumn{1}{c|}{$s$ (\%)} & \multicolumn{1}{c|}{$t$ (ms)} & \multicolumn{1}{c|}{$k$} & \multicolumn{1}{c|}{$f_n$} & \multicolumn{1}{c|}{$t_{LS}$ (ms)} & \multicolumn{1}{c|}{$k_{LS}$} \\
Constant search & 100.0 & 8.3 & 14.0 & 45.0 & 0.3 & 0.0 \\
Golden section search & 100.0 & 92.4 & 17.3 & 808.4 & 80.5 & 376.8 \\
Bisection search & 100.0 & 131.7 & 11.8 & 223.7 & 124.6 & 185.4 \\
Dichotomous search & 100.0 & 68.8 & 11.8 & 486.5 & 60.9 & 224.1 \\
Fibonacci search & 100.0 & 72.1 & 11.8 & 843.4 & 64.3 & 390.8 \\
Uniform search & 100.0 & 134.9 & 12.9 & 1051.1 & 126.6 & 996.3 \\
Newton's search & 100.0 & 541.0 & 12.8 & 806.1 & 533.7 & 250.6 \\
Armijo's search & 100.0 & 15.5 & 14.4 & 113.2 & 6.9 & 23.6 \\
\hline
\end{tabular}
\end{center}


\subsection{Analysis}

There are multiple comparisons we could make using the data we get. Since our goal is to compare the performances of the main methods concerning the choice of the line search method, we display the data primarily by the line search method and secondarily by the main method. We are also interested in spotting any differences between the two problems, so let us also include a comparison between the objective functions.

In addition to the different comparisons, there are also multiple metrics we could look into when analyzing the results. The most important metric for measuring the algorithm's performance is the success rate of solving the problem. However, since most of the methods have similar success rates and the differences in solution percentages are small, we need a different metric for a measure of performance. We choose to use the average solution duration of the main method as our primary comparison metric.

Even though the average solution times of the methods are not universally comparable within different implementations or problems, it provides us a decent comparison of the line search methods since we are using identical scenarios for each main method. Therefore the values in tables \ref{tab:colors_mss_durations} to \ref{tab:colors_avg_overall} are mainly comparable within the same column. To visualize the column-wise differences, we apply a per-column color scale in which red implies poor performance, and green implies good performance. The darker the color, the worse or better the value is within the column. White cells have their value around the average of the column.

\begin{table}[H]
    \centering
    \captionof{table}{Average duration of the algorithms using different line search methods for each main method when finding minimum for the MSS problem. The color scale is applied to visualize the performance per column. The scale goes from dark red to dark green and respectively from the worst (slowest) to the best performance (fastest).}
    \label{tab:colors_mss_durations}
    \begin{tabular}{|l|r|r|r|r|}
    \hline
    \rowcolor[HTML]{C0C0C0} 
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\textbf{Line Search Method}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{NM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{GDM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{CGM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{HBM}} \\ \hline
    Constant search                                                            & 445,80                                                   & \cellcolor[HTML]{E67B73}4 681,20                          & 989,30                                                    & \cellcolor[HTML]{E67B73}4 711,50                          \\ \hline
    Golden section search                                                       & 445,00                                                   & \cellcolor[HTML]{A3DABF}283,70                            & \cellcolor[HTML]{6DC49A}429,50                            & \cellcolor[HTML]{6DC49A}227,70                            \\ \hline
    Bisection search                                                           & \cellcolor[HTML]{E67B73}535,90                           & \cellcolor[HTML]{FCEDEC}823,50                            & \cellcolor[HTML]{E67B73}1 154,90                          & 666,60                                                    \\ \hline
    Dichotomous search                                                         & \cellcolor[HTML]{7BC9A3}429,10                           & \cellcolor[HTML]{A3DABF}270,10                            & \cellcolor[HTML]{6DC49A}426,30                            & \cellcolor[HTML]{7BC9A3}209,40                            \\ \hline
    Fibonacci search                                                           & \cellcolor[HTML]{FCEDEC}453,20                           & \cellcolor[HTML]{F0F9F5}354,30                            & \cellcolor[HTML]{F0F9F5}517,80                            & 353,20                                                    \\ \hline
    Uniform search                                                             & \cellcolor[HTML]{FCEDEC}455,00                           & 389,20                                                    & 558,20                                                    & \cellcolor[HTML]{F0F9F5}322,40                            \\ \hline
    Newton's search                                                             & \cellcolor[HTML]{F0F9F5}443,50                           & 453,30                                                    & 552,80                                                    & 375,10                                                    \\ \hline
    Armijo's search                                                              & \cellcolor[HTML]{7BC9A3}424,20                           & \cellcolor[HTML]{7BC9A3}209,30                            & \cellcolor[HTML]{7BC9A3}405,30                            & \cellcolor[HTML]{7BC9A3}217,30                            \\ \hline
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \captionof{table}{Average duration of the algorithms using different line search methods for each main method when finding minimum for the NE problem. The color scale is applied to visualize the performance per column. The scale goes from dark red to dark green and respectively from the worst (slowest) to the best performance (fastest).}
    \label{tab:colors_ne_durations}
    \begin{tabular}{|l|r|r|r|r|}
    \hline
    \rowcolor[HTML]{C0C0C0} 
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\textbf{Line Search Method}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{NM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{GDM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{CGM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{HBM}} \\ \hline
    Constant search                                                            & \cellcolor[HTML]{E67B73}3 718,10                         & \cellcolor[HTML]{57BB89}13,10                             & \cellcolor[HTML]{EFAAA4}4 393,40                          & \cellcolor[HTML]{57BB89}8,30                              \\ \hline
    Golden section search                                                       & \cellcolor[HTML]{57BB89}2 038,30                         & 53,40                                                     & \cellcolor[HTML]{D7EFE3}539,70                            & 92,40                                                     \\ \hline
    Bisection search                                                           & \cellcolor[HTML]{E5F4ED}2 191,60                         & \cellcolor[HTML]{FDF0EF}92,20                             & 759,50                                                    & \cellcolor[HTML]{FDF0EF}131,70                            \\ \hline
    Dichotomous search                                                         & 2 231,20                                                 & 54,70                                                     & \cellcolor[HTML]{D7EFE3}576,40                            & \cellcolor[HTML]{E5F4ED}68,80                             \\ \hline
    Fibonacci search                                                           & \cellcolor[HTML]{90D2B2}2 100,70                         & 58,40                                                     & \cellcolor[HTML]{E5F4ED}623,70                            & \cellcolor[HTML]{E5F4ED}72,10                             \\ \hline
    Uniform search                                                             & \cellcolor[HTML]{FDF0EF}2 316,00                         & \cellcolor[HTML]{FDF0EF}121,80                            & \cellcolor[HTML]{E67B73}6 371,10                          & \cellcolor[HTML]{FDF0EF}134,90                            \\ \hline
    Newton's search                                                             & 2 266,00                                                 & \cellcolor[HTML]{E67B73}458,80                            & \cellcolor[HTML]{F6CFCB}2 801,70                          & \cellcolor[HTML]{E67B73}541,00                            \\ \hline
    Armijo's search                                                              & \cellcolor[HTML]{E5F4ED}2 206,90                         & \cellcolor[HTML]{57BB89}11,60                             & \cellcolor[HTML]{57BB89}198,40                            & \cellcolor[HTML]{57BB89}15,50                             \\ \hline
    \end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \captionof{table}{The average of the durations for finding the minimum for both the MSS and NE problems. Results are listed separately for each main method. The color scale is applied to visualize the performance per column. The scale goes from dark red to dark green and respectively from the worst (slowest) to the best performance (fastest).}
    \label{tab:colors_avg_overall_mm}
    \begin{tabular}{|l|r|r|r|r|}
    \hline
    \rowcolor[HTML]{C0C0C0} 
    \textbf{Line Search Method} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{NM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{GDM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{CGM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{HBM}} \\ \hline
    Constant search              & \cellcolor[HTML]{E67B73}2 081,95                         & \cellcolor[HTML]{E67B73}2 347,15                          & \cellcolor[HTML]{EDA19C}2 691,35                          & \cellcolor[HTML]{E67B73}2 359,90                          \\ \hline
    Golden section search         & \cellcolor[HTML]{57BB89}1 241,65                         & \cellcolor[HTML]{91D2B2}168,55                            & \cellcolor[HTML]{91D2B2}484,60                            & \cellcolor[HTML]{91D2B2}160,05                            \\ \hline
    Bisection search             & 1 363,75                                                 & 457,85                                                    & 957,20                                                    & 399,15                                                    \\ \hline
    Dichotomous search           & \cellcolor[HTML]{D2ECDF}1 330,15                         & \cellcolor[HTML]{91D2B2}162,40                            & \cellcolor[HTML]{91D2B2}501,35                            & \cellcolor[HTML]{57BB89}139,10                            \\ \hline
    Fibonacci search             & \cellcolor[HTML]{91D2B2}1 276,95                         & \cellcolor[HTML]{D2ECDF}206,35                            & \cellcolor[HTML]{91D2B2}570,75                            & 212,65                                                    \\ \hline
    Uniform search               & 1 385,50                                                 & 255,50                                                    & \cellcolor[HTML]{E67B73}3 464,65                          & 228,65                                                    \\ \hline
    Newton's search               & 1 354,75                                                 & \cellcolor[HTML]{FDF2F1}456,05                            & \cellcolor[HTML]{F7D3D0}1 677,25                          & \cellcolor[HTML]{FDF2F1}458,05                            \\ \hline
    Armijo's search                & \cellcolor[HTML]{D2ECDF}1 315,55                         & \cellcolor[HTML]{57BB89}110,45                            & \cellcolor[HTML]{57BB89}301,85                            & \cellcolor[HTML]{57BB89}116,40                            \\ \hline
    \end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \captionof{table}{The average of the durations for finding the minimum with all the main methods listed separately for each objective function. The color scale is applied to visualize the performance per column. The scale goes from dark red to dark green and respectively from the worst (slowest) to the best performance (fastest).}
    \label{tab:colors_avg_overall}
    \begin{tabular}{|l|r|r|r|}
    \hline
    \rowcolor[HTML]{C0C0C0} 
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\textbf{Line Search Method}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{MSS}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{NE}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{OVERALL}} \\ \hline
    Constant search                                                            & \cellcolor[HTML]{E67B73}2 706,95                          & \cellcolor[HTML]{EDA19C}2 033,23                         & \cellcolor[HTML]{E67B73}2 370,09                              \\ \hline
    Golden section search                                                       & \cellcolor[HTML]{74C79E}346,48                            & \cellcolor[HTML]{A5DABF}680,95                           & \cellcolor[HTML]{74C79E}513,71                                \\ \hline
    Bisection search                                                           & \cellcolor[HTML]{F7D3D0}795,23                            & 793,75                                                   & 794,49                                                        \\ \hline
    Dichotomous search                                                         & \cellcolor[HTML]{74C79E}333,73                            & \cellcolor[HTML]{C9E9D9}732,78                           & \cellcolor[HTML]{74C79E}533,25                                \\ \hline
    Fibonacci search                                                           & 419,63                                                    & \cellcolor[HTML]{C9E9D9}713,73                           & \cellcolor[HTML]{A5DABF}566,68                                \\ \hline
    Uniform search                                                             & 431,20                                                    & \cellcolor[HTML]{E67B73}2 235,95                         & \cellcolor[HTML]{EDA19C}1 333,58                              \\ \hline
    Newton's search                                                             & 456,18                                                    & \cellcolor[HTML]{EDA19C}1 516,88                         & \cellcolor[HTML]{F7D3D0}986,53                                \\ \hline
    Armijo's search                                                              & \cellcolor[HTML]{57BB89}314,03                            & \cellcolor[HTML]{57BB89}608,10                           & \cellcolor[HTML]{57BB89}461,06                                \\ \hline
    \end{tabular}
\end{table}


While the success rate is the primary metric for comparison, it does not offer as interesting differences as the average overall duration. However, it is an essential piece of information to be addressed along with the duration comparison to understand the relations of different line search methods better. The following tables \ref{tab:colors_success_mss} and \ref{tab:colors_success_ne} display the success rates with similar column-wise coloring, but this time with only the below-average cells colored with different shades of red.


\begin{table}[H]
    \centering
    \captionof{table}{The success rates of the main methods using each line search method for finding the minimum of the MSS function. The color scale visualizes the success rate per column so that darker the red, the lower the success rate is.}
    \label{tab:colors_success_mss}
    \begin{tabular}{|l|r|r|r|r|}
    \hline
    \rowcolor[HTML]{C0C0C0} 
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\textbf{Line Search Method}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{NM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{GDM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{CGM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{HBM}} \\ \hline
    Constant search                                                            & 100,00                                                   & \cellcolor[HTML]{E67B73}98,90                             & \cellcolor[HTML]{E67B73}99,60                             & \cellcolor[HTML]{E67B73}98,90                             \\ \hline
    Golden section search                                                       & 100,00                                                   & \cellcolor[HTML]{F7D9D7}99,40                             & 100,00                                                    & \cellcolor[HTML]{F7D9D7}99,40                             \\ \hline
    Bisection search                                                           & \cellcolor[HTML]{E67B73}99,60                            & \cellcolor[HTML]{F7D9D7}99,40                             & 100,00                                                    & \cellcolor[HTML]{F7D9D7}99,40                             \\ \hline
    Dichotomous search                                                         & 100,00                                                   & \cellcolor[HTML]{F7D9D7}99,40                             & 100,00                                                    & \cellcolor[HTML]{F7D9D7}99,40                             \\ \hline
    Fibonacci search                                                           & 100,00                                                   & \cellcolor[HTML]{F7D9D7}99,40                             & 100,00                                                    & \cellcolor[HTML]{F7D9D7}99,40                             \\ \hline
    Uniform search                                                             & 100,00                                                   & \cellcolor[HTML]{F7D9D7}99,40                             & 100,00                                                    & \cellcolor[HTML]{F7D9D7}99,40                             \\ \hline
    Newton's search                                                             & 100,00                                                   & \cellcolor[HTML]{F7D9D7}99,40                             & 100,00                                                    & \cellcolor[HTML]{F7D9D7}99,40                             \\ \hline
    Armijo's search                                                              & 100,00                                                   & 99,60                                                     & 100,00                                                    & 99,60                                                     \\ \hline
    \end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \captionof{table}{The success rates of the main methods using each line search method for finding the minimum of the NE function. The color scale visualizes the success rate per column so that darker the red, the lower the success rate is.}
    \label{tab:colors_success_ne}
    \begin{tabular}{|l|r|r|r|r|}
    \hline
    \rowcolor[HTML]{C0C0C0} 
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\textbf{Line Search Method}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{NM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{GDM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{CGM}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{HBM}} \\ \hline
    Constant search                                                            & 100,00                                                   & 100,00                                                    & \cellcolor[HTML]{E67B73}99,90                             & 100,00                                                    \\ \hline
    Golden section search                                                       & 100,00                                                   & 100,00                                                    & 100,00                                                    & 100,00                                                    \\ \hline
    Bisection search                                                           & 100,00                                                   & 100,00                                                    & 100,00                                                    & 100,00                                                    \\ \hline
    Dichotomous search                                                         & 100,00                                                   & 100,00                                                    & 100,00                                                    & 100,00                                                    \\ \hline
    Fibonacci search                                                           & 100,00                                                   & 100,00                                                    & 100,00                                                    & 100,00                                                    \\ \hline
    Uniform search                                                             & 100,00                                                   & 100,00                                                    & 100,00                                                    & 100,00                                                    \\ \hline
    Newton's search                                                             & 100,00                                                   & 100,00                                                    & 100,00                                                    & 100,00                                                    \\ \hline
    Armijo's search                                                              & 100,00                                                   & 100,00                                                    & 100,00                                                    & 100,00                                                    \\ \hline
    \end{tabular}
\end{table}


In addition to comparing the average performance, an essential factor to consider is the consistency of the results. The boxplots in figures \ref{fig:boxplot_mss} and \ref{fig:boxplot_ne} give us valuable insight into the variance of the algorithms' performance.

The durations in the box plots are scaled logarithmically to improve the readability of the figures in scenarios where some methods perform drastically different from others. The box plot displays half of the data points within the main box area. Together with the box area, the whiskers above and belove the box cover 95\% of the data points. The rest of the points are considered outliers and left out entirely. The orange line inside the boxes shows the exact mean of the data points.


\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{images/boxplot_mss.png}
	\caption{Box plot of solution times per line search and main method for the MSS problem.}
	\label{fig:boxplot_mss}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{images/boxplot_ne.png}
	\caption{Box plot of solution times per line search and main method for the NE problem.}
	\label{fig:boxplot_ne}
\end{figure}

%https://matplotlib.org/3.1.1/gallery/statistics/boxplot_demo.html
%https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51

\subsection{Optimization method comparison}

In this paper, we mainly focus on comparing the line search methods, but it is crucial also to note a few things about the main optimization methods as well. For this comparison, we are mainly going to look at the performance data in tables \ref{tab:performance_results_MSS_NM} to \ref{tab:performance_results_NE_HBM}, as well as the combined results in colored tables \ref{tab:colors_mss_durations}-\ref{tab:colors_success_ne}.

\subsubsection*{Newton's method}

We know that Newton's method has the property that it solves any convex quadratic problem, such as the MSS problem, in one step \cite{book:convex_optimization}. We can see this behavior in table \ref{tab:performance_results_MSS_NM}, where the iterations column has the value $k = 1$ for each line search. We can also see that some line searches have proceeded to execute several iterations, even though the main method would note require that. In this case, a constant step size should be the fastest option since it does not even make the unnecessary function call to the line search algorithm, but we can see that this is not the case. The unexpected difference of ~20-30ms, which the methods mostly share between each other, can be explained by random variation. The fact that the bisection search is also able to cause Newton's method not to converge in 0.4\% of the 1000 starting point setups implies that there is likely some improvements to be made to that line search at least. 

Newton's method generally seems slower for the negative entropy problem. Since the problem is not quadratic, Newton's method needs several iterations with heavy calculations, such as calculating the Hessian, which is a possible cause for the added delay. Newton's method performs well on the MSS problem, but the performance on the NE problem is so weak that Newton's method ends up being the slowest main method in general, as seen in table \ref{tab:colors_avg_overall}.

\subsubsection*{Gradient descent method}

Gradient descent method's performance is known to heavily depend on the condition number of the problem \cite{book:introduction_continuous_optimization}. We discuss the MSS problem's condition number in detail in section \ref{sect:matrix_square_sum}. From the imperfect success rates with the MSS problem, we notice that there are few points in the randomized distribution from which the gradient descent method has problems converging. The effect that the condition number has on the performance is significant and therefore, should be accounted for when analyzing the gradient descent method's performance on such problems.

Even though the gradient method has some problematic starting points, it and the heavy ball method based on the same idea are the best performing optimization methods overall.

\subsubsection*{Conjugate gradient method}

With our problems and the implementation of conjugate gradient method, we expect the algorithm to find a solution in $n=50$ iterations \cite{book:introduction_continuous_optimization}. From the performance tables, we can see that this is mostly the case for the MSS problem. The solution times are, in general, a little higher than with the other methods. The same effect can be seen with the NE problem, though in that case, the iterations required are approximately 100. The unexpectedly high iteration count may be due to the domain limiter causing unexpected inaccuracies within the algorithms. Nevertheless, the algorithm does find the correct solution in almost all cases. Conjugate gradient method has a reliable performance in general but faces serious issues when combined with some line searches and the NE problem in particular.

\subsubsection*{Heavy ball method}

The heavy ball method is based on the gradient descent method and therefore shares its limitations and properties. In theory, the heavy ball method should, at worst, reach the performance of the gradient descent method. We notice that this is mostly the case in the MSS problem, where HBM mostly provides small improvement over the traditional gradient descent method. However, the solution times in the case of the NE problem are mostly slower, implying that the parameter configuration might have been faulty. Since the parameter options did not include option $\beta = 0$, which would have turned HBM into GDM, the values are accepted but not a sign of the method being generally worse.


\subsection{Line search comparison}


To compare the line search methods, we are going to use the same tables as with main optimization methods, but this time focus on the grouping by rows instead of columns.

\subsubsection*{Constant search}

Constant search always gives a constant step size and therefore provides an excellent baseline to compare the other line search methods to. The execution of constant search costs zero time, so the burden of optimization remains on the main method.

Looking at the overall durations of constant search in table \ref{tab:colors_avg_overall} and the success rates in tables \ref{tab:colors_success_mss} and \ref{tab:colors_success_ne}, we can see that the constant search is generally the worst-performing method as expected. The only scenarios where constant step size proves to be rather effective are when optimizing the negative entropy function using gradient descent or heavy ball methods. In those two cases, the constant search was the best or second-best performer. With NE problem gradient descent based methods seem to perform well in general, and since the Constant Search does not require any time to execute, it gives high overall performances as well.

Looking at the box plots in figures \ref{fig:boxplot_mss} and \ref{fig:boxplot_ne}, constant step size is again the worst performer by not just the absolute durations, but also by the high variance of results. The only two exceptions to this are the previously mentioned cases of NE+GD and NE+HBM, where constant search scores one of the lowest variances.

\subsubsection*{Golden section search}

Comparing the overall results in tables \ref{tab:colors_avg_overall_mm} and \ref{tab:colors_avg_overall}, we immediately notice from the green-colored cells that the golden section search is one of the best performing line searches when comparing the total solution times. The method does, like most methods tested, find the correct solution in almost all of the test cases. The variances displayed in the box plots are also average. 

Besides, as stated in section \ref{sect:golden_section_search}, a non-optimal implementation of the golden section search was used, meaning that we could reduce the function call count by up to 50\%. With the problems tested, this would lead to a noticeable but not groundbreaking difference in the method's performance. Even with this mistake, the golden section search proved as one of the three top performers in our tests.

\subsubsection*{Bisection search}

Bisection search does perform rather poorly in our test setup. It scores the second to lowest total success rate, though only losing by 0.4 percentile points to others in the single case of MSS+NM. The overall performance is especially weak with the MSS problem and barely average with other setups. The box plots show that the bisection search does, however, have pretty average variances in performance.

\subsubsection*{Dichotomous search}

In our initial tests, the dichotomous search was one of the slower methods due to not finding the correct solutions within the given maximum iteration limit. However, lowering the main method's tolerance proved to be especially helpful for this line search, which ultimately turned out surprisingly as one of the stronger methods. It even outperforms the golden section search in MSS optimization and scores almost equally in overall scores. The success rates are also identical, but reviewing the variance of the performance from box plots reveals some issues with the dichotomous search when used with Newton's method in particular. Overall, the dichotomous search proved as one of the three top performers in our tests.

\subsubsection*{Fibonacci search}

The Fibonacci search ends up with an average ranking in our comparison in all measures: the raw performance, success rate, and variance. Our implementation of the Fibonacci search computes each Fibonacci number during runtime without saving them into memory. The highest Fibonacci number we reach is around $F_{50}$, so this is not a huge issue. However, for real-life applications, precomputation of the Fibonacci numbers would likely be a worthy addition since it showed around a 10\% increase in performance with only a small increase in memory use.

\subsubsection*{Uniform search}

We expect the uniform search to perform below average due to the inefficiency of the algorithm. From the performance tables, we can see that this is indeed the case, and the uniform search performs averagely on the MSS problem and clearly below average on the NE problem. Part of the poor overall performance in the NE problem is because the method sometimes fails to stay within the 10000 max iteration limit when used with the conjugate gradient method. There are many ways we could presumably relieve these issues, but since we know the uniform search's algorithm is inefficient in its core, it would not make sense to over-optimize its use case.

\subsubsection*{Newton's search}

We find Newton's search to end up on the worse end of our averagely scoring methods on the MSS problem. However, the NE problem is where Newton's line search seems to lack in performance with twice the average overall execution time compared to the other average methods.

Solving the NE problem seems particularly tricky for Newton's method since the line search often hits the max iterations limit, yet it succeeds at finding the correct optimum. This behavior leads us to choose a low max iteration count that allows the method to fail faster when in the scenarios where it would get stuck. We suspect that this special treatment, together with the domain limiter, may be the cause for the method's terrible performance on the NE problem. The unreliability of this method is also visible from the high variances shown in the box plots.

\subsubsection*{Armijo's search}

Armijo's search proves to be the overall best performing method of the tested line searches. It has the best performance measured in the solution times in five categories out of eight and is right on par with the top performers in the rest of the categories as well. It also has a slightly higher success rate on MSS+GD and MSS+HBM scenarios compared to all other methods. There is an average amount of variance in the performances of Armijo's search. However, even after accounting for that, the performance is still clearly better or on-par with the other top performers.

\subsection{Assumptions and further performance improvements}

One major factor to consider when comparing optimization algorithms is the problems used to measure the methods. In this paper, we only used two different problems. The randomization of the parameters adds some variety in the MSS scenario, but still, both of the objective functions remain relatively simple. The randomization in MSS is also modest and does not provide us with many unique problems. Both of the problems considered are also convex and have a single global maximum, making the solving very different from when there also exist local maximums.

In the case of the NE problem, we use a domain limiter algorithm to ensure that values stay within the correct domain. As discussed earlier, this potentially causes some convergence issues with some of the line searches. 

Another factor to consider is the starting point distribution. While we find the starting point count of 1000 to be reasonable, the range from which we generate the points could be set more extensive.

Choosing the optimal parameter values also plays a significant role in the performance comparison. Since choosing the best parameters is a complex problem in itself, we find our approach of finding the parameters by small scale simulations a justified solution for the scope of this thesis at least. However, by increasing the number of options for the values of parameters, the iterations, and the number of test cases, we can likely reach even better results. The tradeoff would naturally be a higher computation time required to find the optimal parameters.

Some performance factors are challenging to evaluate accurately. One example of such a factor is the actual implementation of an optimization algorithm, which seems to have small variances depending on the source. One difference, in particular, seems to be the position of the stopping condition within the main methods. Some sources suggest checking the stopping condition before updating the point, but in all of our implementations, the check is done just after the update before entering the next iteration. \cite{book:convex_optimization} \cite{book:nonlinear_programming}

Another possible implementation related optimization would be the pre-calculation of line search values. In some scenarios, it could be beneficial to pre-evaluate and save the step sizes for each different input instead of calculating them during run time. \cite{book:convex_optimization}

Factors like the choice of programming language, and CPU, GPU, and RAM performance may affect the optimal results as well. In this thesis, we do all of the performance testings using Python 3.7.3 with numpy, autograd, and multiprocessing libraries. One can find the source code used to produce the results from the appendix \ref{sect:appendix}. We run the software on 64bit Linux desktop PC with an Intel i5-8500K (6 cores @ 3.60GHz) CPU and 16 GB of RAM. Different implementations and programming languages may, for example, optimize matrix calculations differently, causing potentially unexpected differences within otherwise identical test scenarios. For this reason alone, we consider none of the results as universally accurate and instead suggest evaluating each problem and scenario separately.


\section{Conclusion}
\label{sect:conclusion}


The choice of line search method seems to have a varying size of an impact on the performance of an optimization method. For example, comparing just the average overall durations for different line searches, it seems that the worst line search is several times slower than the best one. The top three methods, in turn, are around 10-20 \% slower than the fastest one in average overall duration. 

If we were to perform this research again and the performance comparisons again, we would use a higher number and variety of objective functions with a broader range of starting points. We would also like to spend more time justifying the different choices for the parameters of the optimization methods. For example, we could try to find proven values from existing literature or simply spend more time evaluating a more extensive range of different values. One change that would help in re-evaluating the parameters would be to drop out uninteresting line searches. Leaving out the slowest methods could offer significant time savings since evaluating algorithms that rarely converge is very time inefficient. 

Is it worth it to optimize the choice of line search method as an additional parameter for an unconstrained nonlinear optimization method? If one is already using a high-performance line search method such as Armijo's search or golden section search, it is unlikely that any of the tested alternative algorithms would provide any improvement in performance. The only scenario in which there could potentially be a better alternative would be when the main optimization method solves the problem efficiently without a line search method, namely using a constant step size. However, this is a rare occurrence that would require a specific type of problem and optimization algorithm.



% The choice of line search method seems to have a varying size of an impact on the performance of an optimization method. Comparing just the average overall durations for different line searches, it seems that the worst line search is several times slower than the best one. The difference in performance for the best and second-best method is around 11 \%, and the third one is around 16 \% slower than the fastest one. These numbers are not final by any means but instead, give an idea of the range of improvements we can look for when considering the choice of a line search algorithm.

% If we were to perform further testing with the same goals, we would select a higher number and variety of objective functions and a wider range of starting points. However, there is even more constraining issue we would need to tackle first: the optimal parameters are likely to change with the setups, forcing us to re-run the parameter comparisons and find new values for all the parameters. Since this requires high computation times, we should implement a better method for parameter selection before considering any new test scenarios. A simple solution could be to decide a well-reasoned set of options for each parameter based more strictly on existing literature around the topics.

% Is it worth it to optimize the choice of line search method as an additional parameter for an unconstrained nonlinear optimization method? If one is already using a high-performance line search method such as Armijo's search, there is at most a minimal chance of gaining any benefit by performing a line search comparison. However, we do believe that the subject could use further research to make the results more comprehensive and to build a framework for comparing and scoring line search methods in different scenarios.
% , but with the current results, we can show that there are indeed performance differences between the line searches. However, the results are not surprising, and the methods that we generally consider to be efficient are, in fact, the best choices in most use cases. There might exist problems for which some algorithms fit better than others, and therefore if the goal is to optimize a single problem as thoroughly as possible, the choice of line search method might be worth considering as an additional parameter. As long as a decent line search method is used in the first place, optimizing its parameters might be a better idea than looking for an alternative line search algorithm.

% %
% % LÄHDELUETTELO
% %  BibTeX-tiedoston kokoaminen onnistuu näppärästi esim. 
% %  Firefoxin Zotero-lisäosalla http://www.zotero.org/
% %  joka osaa poimia viitteet suoraan Google Scholarista.
% %
\newpage
% \bibliographystyle{plainnat}
\bibliographystyle{abbrvnat}
%\bibliographystyle{plain} % valitse tämä jos et käytä natbib-pakettia

% Ladataan library.bib
\bibliography{library}


% %
% % LIITTEET
% %

\newpage
\appendix
\section{Appendix}
\label{sect:appendix}

\subsection{Source code}
All source code in full is available in Github: \\
\url{https://github.com/EinariTuukkanen/line-search-comparison}

\subsection{Parameter comparison results}
% TODO: enable
\input{parameters.tex}


\end{document}
